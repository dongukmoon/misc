{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFaKC4ei3iKQ"
      },
      "source": [
        "\n",
        "# vLLM Tutorial: Quick Start on Google Colab\n",
        "This guide walks you through setting up and running offline batched inference with vLLM on Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running Offline Batched Inference on Colab**\n",
        "\n",
        "Step 1: Install Required Packages\n",
        "\n",
        "Start by installing vLLM and other necessary packages using pip:"
      ],
      "metadata": {
        "id": "fZVPiHkjEs7Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BU1yI1tBbrA",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install vllm torch  triton"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Set Up GPU Environment**\n",
        "\n",
        "Google Colab offers free access to a Tesla T4 GPU (15GB RAM), which is sufficient for running vLLM. In this tutorial, we'll use the GPT-2 model and set the `gpu_memory_utilization` parameter to 0.5. This ensures that the key-value cache reserved for model inference does not exceed 50% of the total GPU memory. Without this setting, you might encounter out-of-memory errors due to the large key-value cache required for the maximum number of tokens supported by GPT-2.\n",
        "With `gpu_memory_utilization`=0.5, the model weights will use about 0.24GB, and the key-value cache will require approximately 6.52GB of GPU memory.\n",
        "\n",
        "\n",
        "\n",
        "**Step 3: Load the Model and Generate Outputs**\n",
        "\n",
        "Below is the Python script to load the GPT-2 model, define prompts, and generate text outputs:"
      ],
      "metadata": {
        "id": "vxPTtuXxE64o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLAn4PIG79r2"
      },
      "outputs": [],
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# Load a pre-trained Hugging Face model\n",
        "model_name = \"gpt2\"  # Replace with your desired model\n",
        "\n",
        "# Create an LLM.\n",
        "#gpu_memory_utilization: The ratio (between 0 and 1) of GPU memory to\n",
        "#reserve for the model weights, activations, and KV cache. Higher\n",
        "#values will increase the KV cache size and thus improve the model's\n",
        "#throughput. However, if the value is too high, it may cause out-of-\n",
        "# memory (OOM) errors.\n",
        "llm = LLM(model_name, gpu_memory_utilization = 0.5)\n",
        "\n",
        "# Sample prompts.\n",
        "prompts = [\n",
        "    \"Hello, my name is\",\n",
        "    \"The president of the United States is\",\n",
        "    \"The capital of France is\",\n",
        "    \"The future of AI is\",\n",
        "]\n",
        "# Create a sampling params object.\n",
        "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
        "\n",
        "# Generate texts from the prompts. The output is a list of RequestOutput objects\n",
        "# that contain the prompt, generated text, and other information.\n",
        "outputs = llm.generate(prompts, sampling_params)\n",
        "# Print the outputs.\n",
        "for output in outputs:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Monitor GPU Memory Usage**\n",
        "\n",
        "Use the nvidia-smi command to check GPU memory usage. As shown, approximately 7.5GB out of 15GB of GPU memory is used."
      ],
      "metadata": {
        "id": "YqMI1NRBFVmf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LRzy7A5AhZA",
        "outputId": "82f4f573-712f-4514-ee27-4ba2384ef1c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec  1 04:51:32 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0              26W /  70W |   7525MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample Output**\n",
        "\n",
        "The output of the script may look like this:\n",
        "\n",
        "```\n",
        "Prompt: 'Hello, my name is', Generated text: ' Scott. If you want to talk to me, you can. I am a'\n",
        "Prompt: 'The president of the United States is', Generated text: ' a trained lawyer with experience in working with the vast majority of lawyers who are part'\n",
        "Prompt: 'The capital of France is', Generated text: ' on the basis of the basis of the shared law. This is called the Common'\n",
        "Prompt: 'The future of AI is', Generated text: ' becoming a bit more difficult. The big, big picture lies with AI. We'\n",
        "```"
      ],
      "metadata": {
        "id": "vWmU2LiQF6G6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Handle GPU Memory for Repeated Runs**\n",
        "\n",
        "If you run the script again without restarting the Colab instance, you might encounter out-of-memory errors. To free up GPU memory, you can simply restart the Colab environment using the following command:"
      ],
      "metadata": {
        "id": "X06-srRFFkMJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGWPhpYcAaar"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os._exit(00)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more details, refer to the vLLM Documentation."
      ],
      "metadata": {
        "id": "n3tVDiKQFuHD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zidwp2hJFycp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMWATE51igP52UNJrM9W/hY"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}