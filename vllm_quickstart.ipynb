{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFaKC4ei3iKQ"
      },
      "source": [
        "# vLLM Quickstart\n",
        "\n",
        "## Offline Batched Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BU1yI1tBbrA",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install vllm torch  triton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLAn4PIG79r2"
      },
      "outputs": [],
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# Load a pre-trained Hugging Face model\n",
        "model_name = \"gpt2\"  # Replace with your desired model\n",
        "\n",
        "# Create an LLM.\n",
        "#gpu_memory_utilization: The ratio (between 0 and 1) of GPU memory to\n",
        "#reserve for the model weights, activations, and KV cache. Higher\n",
        "#values will increase the KV cache size and thus improve the model's\n",
        "#throughput. However, if the value is too high, it may cause out-of-\n",
        "# memory (OOM) errors.\n",
        "llm = LLM(model_name, gpu_memory_utilization = 0.5)\n",
        "\n",
        "# Sample prompts.\n",
        "prompts = [\n",
        "    \"Hello, my name is\",\n",
        "    \"The president of the United States is\",\n",
        "    \"The capital of France is\",\n",
        "    \"The future of AI is\",\n",
        "]\n",
        "# Create a sampling params object.\n",
        "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
        "\n",
        "# Generate texts from the prompts. The output is a list of RequestOutput objects\n",
        "# that contain the prompt, generated text, and other information.\n",
        "outputs = llm.generate(prompts, sampling_params)\n",
        "# Print the outputs.\n",
        "for output in outputs:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e75D-VPw_ldQ"
      },
      "source": [
        "Restart Colab to free GPU memory and rerun the demo above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGWPhpYcAaar"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#os._exit(00)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LRzy7A5AhZA",
        "outputId": "82f4f573-712f-4514-ee27-4ba2384ef1c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec  1 04:51:32 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0              26W /  70W |   7525MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPvheoPjFpj1p1wUSLrvxJ4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}