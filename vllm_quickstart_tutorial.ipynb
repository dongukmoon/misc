{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dongukmoon/misc/blob/main/vllm_quickstart_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFaKC4ei3iKQ"
      },
      "source": [
        "\n",
        "# vLLM Tutorial: Quick Start on Google Colab\n",
        "This guide walks you through setting up and running offline batched inference and OpenAI-compatible inference with vLLM on Google Colab.\n",
        "\n",
        "\n",
        "1.   Offline batched inference\n",
        "2.   OpenAI-compatible inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Offline Batched Inference\n",
        "\n",
        "**Step 1: Install Required Packages**\n",
        "\n",
        "Start by installing vLLM and other necessary packages using pip:"
      ],
      "metadata": {
        "id": "fZVPiHkjEs7Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BU1yI1tBbrA",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install vllm torch  triton"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Set Up GPU Environment**\n",
        "\n",
        "Google Colab offers free access to a Tesla T4 GPU (15GB RAM), which is sufficient for running vLLM. In this tutorial, we'll use the GPT-2 model and set the `gpu_memory_utilization` parameter to 0.5. This ensures that the key-value cache reserved for model inference does not exceed 50% of the total GPU memory. Without this setting, you might encounter out-of-memory errors due to the large key-value cache required for the maximum number of tokens supported by GPT-2.\n",
        "With `gpu_memory_utilization`=0.5, the model weights will use about 0.24GB, and the key-value cache will require approximately 6.52GB of GPU memory.\n",
        "\n",
        "\n",
        "\n",
        "**Step 3: Load the Model and Generate Outputs**\n",
        "\n",
        "Below is the Python script to load the GPT-2 model, define prompts, and generate text outputs:"
      ],
      "metadata": {
        "id": "vxPTtuXxE64o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLAn4PIG79r2"
      },
      "outputs": [],
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# Load a pre-trained Hugging Face model\n",
        "model_name = \"gpt2\"  # Replace with your desired model\n",
        "\n",
        "# Create an LLM.\n",
        "#gpu_memory_utilization: The ratio (between 0 and 1) of GPU memory to\n",
        "#reserve for the model weights, activations, and KV cache. Higher\n",
        "#values will increase the KV cache size and thus improve the model's\n",
        "#throughput. However, if the value is too high, it may cause out-of-\n",
        "# memory (OOM) errors.\n",
        "llm = LLM(model_name, gpu_memory_utilization = 0.5)\n",
        "\n",
        "# Sample prompts.\n",
        "prompts = [\n",
        "    \"Hello, my name is\",\n",
        "    \"The president of the United States is\",\n",
        "    \"The capital of France is\",\n",
        "    \"The future of AI is\",\n",
        "]\n",
        "# Create a sampling params object.\n",
        "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
        "\n",
        "# Generate texts from the prompts. The output is a list of RequestOutput objects\n",
        "# that contain the prompt, generated text, and other information.\n",
        "outputs = llm.generate(prompts, sampling_params)\n",
        "# Print the outputs.\n",
        "for output in outputs:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Monitor GPU Memory Usage**\n",
        "\n",
        "Use the nvidia-smi command to check GPU memory usage. As shown, approximately 7.5GB out of 15GB of GPU memory is used."
      ],
      "metadata": {
        "id": "YqMI1NRBFVmf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LRzy7A5AhZA"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample Output**\n",
        "\n",
        "The output of the script may look like this:\n",
        "\n",
        "```\n",
        "Prompt: 'Hello, my name is', Generated text: ' Scott. If you want to talk to me, you can. I am a'\n",
        "Prompt: 'The president of the United States is', Generated text: ' a trained lawyer with experience in working with the vast majority of lawyers who are part'\n",
        "Prompt: 'The capital of France is', Generated text: ' on the basis of the basis of the shared law. This is called the Common'\n",
        "Prompt: 'The future of AI is', Generated text: ' becoming a bit more difficult. The big, big picture lies with AI. We'\n",
        "```"
      ],
      "metadata": {
        "id": "vWmU2LiQF6G6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Handle GPU Memory for Repeated Runs**\n",
        "\n",
        "If you run the script again without restarting the Colab instance, you might encounter out-of-memory errors. To free up GPU memory, you can simply restart the Colab environment using the following command:"
      ],
      "metadata": {
        "id": "X06-srRFFkMJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGWPhpYcAaar"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os._exit(00)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running OpenAI-Compatible Inference\n",
        "vLLM can act as a server implementing the OpenAI API protocol, making it a drop-in replacement for applications using OpenAI APIs. The default server address is http://localhost:8000. You can customize it with the --host and --port arguments. The server hosts one model at a time and supports endpoints such as list models, create completion, and create chat completion.\n",
        "\n",
        "**Starting the vLLM Server**\n",
        "\n",
        "Run the following command to start the vLLM server with the Qwen/Qwen2.5-1.5B-Instruct model:\n"
      ],
      "metadata": {
        "id": "kP2urqE6LH1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cannot use FlashAttention-2 backend for Volta and Turing GPUs\n",
        "# and T4 does not support BF16, causing error.\n",
        "# So, I used a L4 GPU instance\n",
        "\n",
        "# wait until the server bootup is complete. Check the log in 'nohup.out'\n",
        "!nohup vllm serve Qwen/Qwen2.5-1.5B-Instruct --port 8001 --gpu-memory-utilization 0.3 &\n"
      ],
      "metadata": {
        "id": "uX946nBzLmia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  This server can be queried in the same format as OpenAI API. For example, to list the models:"
      ],
      "metadata": {
        "id": "EdegLgX8-dmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl http://localhost:8001/v1/models"
      ],
      "metadata": {
        "id": "yTjaSHy2Onzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once your server is started, you can query the model with input prompts:"
      ],
      "metadata": {
        "id": "lV1ykUHA-mrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl http://localhost:8001/v1/completions \\\n",
        "-H \"Content-Type: application/json\" \\\n",
        "-d '{\"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\"prompt\": \"San Francisco is a\", \\\n",
        "\"max_tokens\": 7, \\\n",
        "\"temperature\": 0 \\\n",
        "}'"
      ],
      "metadata": {
        "id": "7sJDrm0CSMwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using the OpenAI Python Package**\n",
        "\n",
        "vLLM's API server is OpenAI-compatible, allowing you to use the OpenAI Python package:\n",
        "\n",
        "**Text Completion**\n",
        "\n"
      ],
      "metadata": {
        "id": "8ubBIMXn_Bx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
        "openai_api_key = \"EMPTY\"\n",
        "openai_api_base = \"http://localhost:8001/v1\"\n",
        "client = OpenAI(\n",
        "    api_key=openai_api_key,\n",
        "    base_url=openai_api_base,\n",
        ")\n",
        "completion = client.completions.create(model=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "                                      prompt=\"San Francisco is a\")\n",
        "print(\"Completion result:\", completion)"
      ],
      "metadata": {
        "id": "Bt_0awKe-uKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chat Completion**\n",
        "\n",
        "vLLM also supports the OpenAI Chat Completions API, enabling a dynamic and interactive way to communicate with the model. Through the chat interface, users can engage in back-and-forth exchanges, with context preserved via chat history. This feature is particularly useful for tasks that demand continuity, detailed explanations, or maintaining context across multiple interactions."
      ],
      "metadata": {
        "id": "f5zb2QuVDfvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "# Set OpenAI's API key and API base to use vLLM's API server.\n",
        "openai_api_key = \"EMPTY\"\n",
        "openai_api_base = \"http://localhost:8001/v1\"\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=openai_api_key,\n",
        "    base_url=openai_api_base,\n",
        ")\n",
        "\n",
        "chat_response = client.chat.completions.create(\n",
        "    model=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
        "    ]\n",
        ")\n",
        "print(\"Chat response:\", chat_response)"
      ],
      "metadata": {
        "id": "ey8Onv-w_bW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "*   https://docs.vllm.ai/en/latest/getting_started/quickstart.html\n",
        "\n",
        "*   https://docs.vllm.ai/en/stable/models/engine_args.html"
      ],
      "metadata": {
        "id": "rnQQK8ad8ZJn"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPfiBynd0uRooqSB68lC2eZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}